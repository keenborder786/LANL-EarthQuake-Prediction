{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "import pickle\n",
    "os.chdir(r'C:\\Users\\MMOHTASHIM\\Anaconda3\\libs\\Small Data Science projects\\Small-Data-Science-Projects\\LANL Earthuquake prediction')\n",
    "model=load_model('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2624 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_31_input to have shape (23,) but got array with shape (22,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7514c2445592>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m       ('scaler', StandardScaler())])\n\u001b[0;32m     51\u001b[0m     \u001b[0mX_transformed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreprocessing_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_transformed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[0mpredicted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1094\u001b[0m       \u001b[1;31m# batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m       x, _, _ = self._standardize_user_data(\n\u001b[1;32m-> 1096\u001b[1;33m           x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[0;32m   1097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m     if (self.run_eagerly or (isinstance(x, iterator_ops.EagerIterator) and\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[0;32m   2380\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2382\u001b[1;33m         exception_prefix='input')\n\u001b[0m\u001b[0;32m   2383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2384\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    360\u001b[0m                 \u001b[1;34m'Error when checking '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m                 ' but got array with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    363\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_31_input to have shape (23,) but got array with shape (22,)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy.stats import kurtosis,skew\n",
    "from statistics import mode,mean\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "os.chdir(r'C:\\Users\\MMOHTASHIM\\Anaconda3\\libs\\Small Data Science projects\\Small-Data-Science-Projects\\LANL Earthuquake prediction\\test_data')\n",
    "predicted={}\n",
    "f=[]\n",
    "for file in os.listdir(os.getcwd()):\n",
    "    f.append(file)\n",
    "for filename in tqdm(f):\n",
    "    df=pd.read_csv(filename)\n",
    "    for newcol in [\"Mean per 150k\",\"std per 150k\",\"min per 150k\",\"max per 150k\",\"Median per 150k\",\n",
    "              \"Mean\",\"std\",\"min\",\"max\",\"Median\",\"Moving Average Overallwindow5\",\n",
    "              \"Moving Average Overallwindow10\",'Ratio Succession_window5','Ratio Succession_window10',\n",
    "              'kurtosis per 150k',\"Mode per 150k\",\"skew per 150k\",\"quantile0.95 per 150k\",\"quantile0.05 per 150k\",\"quantile0.25 per 150k\",\"quantile0.75 per 150k\"]:\n",
    "        df[newcol]=0\n",
    "    df['Mean per 150k']=df[\"acoustic_data\"].rolling(15).mean()\n",
    "    df['std per 150k']=df[\"acoustic_data\"].rolling(15).std()\n",
    "    df['min per 150k']=df[\"acoustic_data\"].rolling(15).max()\n",
    "    df['max per 150k']=df[\"acoustic_data\"].rolling(15).min()\n",
    "    df['Median per 150k']=df[\"acoustic_data\"].rolling(15).median()\n",
    "    df['Mean']=np.mean(df[\"acoustic_data\"])\n",
    "    df['std']=np.std(df[\"acoustic_data\"])\n",
    "    df['min']=np.min(df[\"acoustic_data\"])\n",
    "    df['max']=np.max(df[\"acoustic_data\"])\n",
    "    df['Median']=np.median(df[\"acoustic_data\"])\n",
    "    df[\"Moving Average Overallwindow5\"]=df[\"acoustic_data\"].rolling(5).mean()\n",
    "    df[\"Moving Average Overallwindow10\"]=df[\"acoustic_data\"].rolling(10).mean()\n",
    "    df['Ratio Succession_window5']=df[\"acoustic_data\"].pct_change(periods=5)\n",
    "    df['Ratio Succession_window10']=df[\"acoustic_data\"].pct_change(periods=10)\n",
    "    df['kurtosis per 150k']=kurtosis(df[\"acoustic_data\"])\n",
    "    try:\n",
    "         df[\"Mode per 150k\"]=mode(df[\"acoustic_data\"])\n",
    "    except:\n",
    "         df[\"Mode per 150k\"]=0\n",
    "    df['skew per 150k']=skew(df[\"acoustic_data\"])\n",
    "    df['quantile0.95 per 150k']=np.quantile(df[\"acoustic_data\"],0.95)\n",
    "    df['quantile0.05 per 150k']=np.quantile(df[\"acoustic_data\"],0.05)\n",
    "    df['quantile0.25 per 150k']=np.quantile(df[\"acoustic_data\"],0.25)\n",
    "    df['quantile0.75 per 150k']=np.quantile(df[\"acoustic_data\"],0.75)\n",
    "    \n",
    "    \n",
    "    df=df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    X=np.array(df)\n",
    "    preprocessing_pipeline = Pipeline(steps=[\n",
    "      ('scaler', StandardScaler())])\n",
    "    X_transformed=preprocessing_pipeline.fit_transform(X)  \n",
    "    y_pred=np.array(model.predict(X_transformed))\n",
    "    predicted[str(filename)]=np.mean(y_pred)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\MMOHTASHIM\\Anaconda3\\libs\\Small Data Science projects\\Small-Data-Science-Projects\\LANL Earthuquake prediction')\n",
    "df_sub=pd.read_csv(\"sample_submission.csv\")\n",
    "s=-1\n",
    "for i in df_sub[\"seg_id\"]:\n",
    "         s+=1\n",
    "         df_sub.iloc[s,1]=predicted[str(i)+\".csv\"]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
